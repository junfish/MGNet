{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGNets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hjroS3F7QFKm",
        "he5apTpj-Xpd",
        "YW3I6sqLaM9t",
        "2kc7pYsV8Vjd",
        "SAZX1cpaZcgE",
        "fmdTWfUqewgv",
        "leEAeQ63SOrE",
        "fMcLAK5xQO_n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMriJIWs88S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/MGNets/')\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjroS3F7QFKm",
        "colab_type": "text"
      },
      "source": [
        "### Prepare for Input\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjCW_ZMAQHFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn, sklearn.datasets\n",
        "import sklearn.naive_bayes, sklearn.linear_model, sklearn.svm, sklearn.neighbors, sklearn.ensemble\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse\n",
        "import numpy as np\n",
        "import time, re\n",
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "from  scipy import *\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "def data_projection(data, U):\n",
        "  U_trans = np.transpose(U)\n",
        "  M_U,_ = U_trans.shape\n",
        "  N,V,M,F = data.shape\n",
        "  new_data = np.zeros([N,V,M_U,F])\n",
        "  for i in range(N):\n",
        "    for v in range(V):\n",
        "      data_iv = data[i,v,:,:]\n",
        "      data_iv_trans = np.matmul(U_trans,data_iv)\n",
        "      # data_iv_trans = np.matmul(data_iv_trans, U)\n",
        "      new_data[i,v,:,:] = data_iv_trans\n",
        "\n",
        "  return new_data    \n",
        "\n",
        "\n",
        "\n",
        "def convert_minus_one_2_zero(label):\n",
        "  for i in range(len(label)):\n",
        "    if(label[i]<0):\n",
        "      label[i] = 0;\n",
        "  return label    \n",
        "\n",
        "def load_data(f):\n",
        "  k = 0\n",
        "  input_data = {}\n",
        "\n",
        "  for line in f:\n",
        "    input_data[k] = np.int32(line.split())\n",
        "    k = k + 1\n",
        "  \n",
        "  return input_data\n",
        "\n",
        "\n",
        "def load_train_index(f):\n",
        "  train_index = []\n",
        "  k = 0\n",
        "  for line in f:\n",
        "    train_index.append(np.int32(line))\n",
        "\n",
        "  return train_index\n",
        "    \n",
        "\n",
        "\n",
        "def load_data_my_new(data_name, train_fold_chosen):\n",
        "  # load data from google drive\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "    label = x['label']\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "    label = x['label']\n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "    label = x['label']\n",
        "\n",
        "  # reshape data_size to N,V,M,F  \n",
        "  N_subject = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  print('loading data:',data_name,'of size:',N_subject,V,M,F)\n",
        "  data = np.zeros([N_subject,V,M,F])\n",
        "  for i in range(N_subject):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[i,j,:,:] = X_ij\n",
        "\n",
        "  labels = np.array(label)\n",
        "  if(data_name == 'BP'):\n",
        "    f_test = open('results/BP_divide_test_index.txt','r')\n",
        "    test_idx = load_data(f_test)\n",
        "    f_train = open('results/BP_divide_train_index.txt','r')\n",
        "    train_idx = load_data(f_train)\n",
        "  elif(data_name == 'HIV'):\n",
        "    f_test = open('results/HIV_divide_test_index.txt','r')\n",
        "    test_idx = load_data(f_test)\n",
        "    f_train = open('results/HIV_divide_train_index.txt','r')\n",
        "    train_idx = load_data(f_train)\n",
        "  elif(data_name == 'PPMI'):\n",
        "    f_test = open('results/PPMI_divide_test_index.txt','r')\n",
        "    test_idx = load_data(f_test)\n",
        "    f_train = open('results/PPMI_divide_train_index.txt','r')\n",
        "    train_idx = load_data(f_train)  \n",
        "\n",
        "  f_test.close()\n",
        "  f_train.close()\n",
        "\n",
        "  train_set_ratio = 0.8\n",
        "  train_chosen_idx = train_idx[train_fold_chosen]\n",
        "  data_size = train_chosen_idx.size;  \n",
        "  train_data_size = np.floor(data_size*train_set_ratio)\n",
        "  train_index = random.sample(list(train_chosen_idx), int(train_data_size))\n",
        "  test_index = np.setdiff1d(train_chosen_idx,train_index)\n",
        "\n",
        "  labels_set = list()\n",
        "  indexes_set = list()\n",
        "\n",
        "  train_label, test_label = labels[train_index], labels[test_index]\n",
        "  val_label,val_index = test_label,test_index\n",
        "  train_label = convert_minus_one_2_zero(train_label)\n",
        "  val_label = convert_minus_one_2_zero(val_label)\n",
        "  test_label = convert_minus_one_2_zero(test_label)\n",
        "\n",
        "  train_label = np.array(train_label).flatten()\n",
        "  val_label = np.array(val_label).flatten()\n",
        "  test_label = np.array(test_label).flatten()\n",
        "\n",
        "  train_index = np.array(train_index).flatten()\n",
        "  val_index = np.array(val_index).flatten()\n",
        "  test_index = np.array(test_index).flatten()\n",
        "\n",
        "  labels_set.append((train_label,val_label,test_label))\n",
        "  indexes_set.append((train_index,val_index,test_index))\n",
        "\n",
        "  train_index_all = train_idx\n",
        "  test_index_all = test_idx\n",
        "\n",
        "  subj = 0\n",
        "  return data, subj, indexes_set, labels_set, train_index_all, test_index_all, convert_minus_one_2_zero(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he5apTpj-Xpd",
        "colab_type": "text"
      },
      "source": [
        "### Re-implement graph.py to construct ajacency matrix A and normalized laplacian matrix L_hat (Graph.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K50k1OCV-XDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### re-implement graph.py to construct ajacency matrix A and normalized laplacian matrix L_hat\n",
        "\n",
        "import sklearn.metrics\n",
        "import sklearn.neighbors\n",
        "import scipy\n",
        "import numpy as np\n",
        "from scipy.sparse import rand\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import isspmatrix\n",
        "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Kif's code\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return adj_normalized\n",
        "\n",
        "\n",
        "### construct ajacency matrix\n",
        "def compute_dist(graph,k,metric):\n",
        "  dist = scipy.spatial.distance.pdist(graph,metric)\n",
        "  dist_square = scipy.spatial.distance.squareform(dist)\n",
        "  id_dist = np.argsort(dist_square)[:,:k+1]\n",
        "  dist_square.sort()\n",
        "  dist_square = dist_square[:,:k+1]\n",
        "  return dist_square,id_dist\n",
        "\n",
        "\n",
        "def build_ajacency(dist,idx):\n",
        "  M,k = dist.shape\n",
        "  sigma = np.mean(dist[:,-1])**2\n",
        "  dist = np.exp(-dist/sigma)\n",
        "\n",
        "  # construct sparse matrix\n",
        "  I = np.arange(0,M).repeat(k)\n",
        "  J = idx.reshape(M*k)\n",
        "  V = dist.reshape(M*k)\n",
        "  W = scipy.sparse.coo_matrix((V,(I,J)), shape = (M,M))\n",
        "  W.setdiag(0)\n",
        "  # construct symmetric matrix\n",
        "  bigger_index = W.T>W\n",
        "  # W = W - W.multiply(bigger_index) + W.T.multiply(bigger_index)\n",
        "  W = W + W.T.multiply(bigger_index)\n",
        "  assert(type(W)) is scipy.sparse.csr.csr_matrix\n",
        "  \n",
        "  return W\n",
        "\n",
        "def build_laplacian(W, normalized = True):\n",
        "  d = W.sum(axis = 0)\n",
        "  if not normalized:\n",
        "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
        "    L = D - W\n",
        "  else:   \n",
        "    d += np.spacing(np.array(0,W.dtype))\n",
        "    d = 1/np.sqrt(d)\n",
        "    D = scipy.sparse.diags(d.A.squeeze(),0)\n",
        "    I = scipy.sparse.identity(d.size,dtype= W.dtype)\n",
        "    L = I - D*W*D\n",
        "    # np.savetxt(r'results/PPMI/PPMI_Laplacian.txt',L.todense(),fmt=\"%f\")\n",
        "    # print('I am writing！！！！')\n",
        "    #L = scipy.sparse.rand(d.size, d.size, density=0.25, format=\"csr\", random_state=42)\n",
        "\n",
        "  # assert(type(L)) is scipy.sparse.csr_matrix\n",
        "  return L\n",
        "\n",
        "def rescale_L(L, l_max):\n",
        "  L = 2/l_max * L\n",
        "  I = scipy.sparse.identity(L.shape[0], format='csr', dtype=L.dtype)\n",
        "  L_hat = L - I\n",
        "  return L_hat\n",
        "\n",
        "\n",
        "def obtain_normalized_adj(node_features, k, metric = 'euclidean'):\n",
        "    dist, idx = compute_dist(node_features, k, metric = 'euclidean')\n",
        "    A = build_ajacency(dist, idx).astype(np.float32)\n",
        "    I = scipy.sparse.eye(A.shape[0],dtype= A.dtype)\n",
        "    A_hat = A + I\n",
        "\n",
        "    d = A_hat.sum(axis = 0)\n",
        "    d += np.spacing(np.array(0,A_hat.dtype))\n",
        "    d = 1/np.sqrt(d)\n",
        "    D = scipy.sparse.diags(d.A.squeeze(),0)\n",
        "    normalized_A = D*A_hat*D\n",
        "\n",
        "    return normalized_A\n",
        "\n",
        "\n",
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"\n",
        "    Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\n",
        "    \"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)\n",
        "    \n",
        "# node_features = np.random.rand(4,4)\n",
        "# k = 2\n",
        "# normalized_A = obtain_normalized_adj(node_features,k)## Either code works fine to obtain the normalzed A\n",
        "# normalized_A2 = preprocess_adj(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW3I6sqLaM9t",
        "colab_type": "text"
      },
      "source": [
        "### Initialization Weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZrvsLwzaOUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "initializer = tf.initializers.glorot_uniform()\n",
        "\n",
        "def zeros(shape, name=None):\n",
        "    initial = tf.zeros(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name, trainable = True, dtype=tf.float32)\n",
        "\n",
        "def get_weight( shape , name ='None' ):\n",
        "    # return tf.Variable(  tf.random.truncated_normal(shape,stddev=0.1) , name= name , trainable = True , dtype=tf.float32 )\n",
        "    return tf.Variable(  initializer(shape) , name = name, trainable=True , dtype=tf.float32 )\n",
        "\n",
        "def initialize_bias_weights(shapes):\n",
        "  weights = []\n",
        "  for i in range( len( shapes ) ):\n",
        "    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i )) )\n",
        "\n",
        "  bias = []  \n",
        "  for i in range(len(shapes)):\n",
        "    bias_shape = shapes[i][1] \n",
        "    bias_i = zeros(bias_shape, 'bias{}'.format( i ))\n",
        "    bias.append(bias_i)\n",
        "\n",
        "  return weights, bias  \n",
        "\n",
        "def initialize_shape(num_layers, Input_dim, Output_dim, M, process):\n",
        "  shapes = []\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    if(i == 0):\n",
        "      shapes_i = [Input_dim, Output_dim[0]]\n",
        "    elif(i == num_layers - 1):\n",
        "      if(process == 'concat'):\n",
        "        shapes_i = [M * Output_dim[i-1], Output_dim[i]]\n",
        "      else:\n",
        "        shapes_i = [Output_dim[i-1], Output_dim[i]]\n",
        "\n",
        "    else:\n",
        "      shapes_i = [Output_dim[i-1], Output_dim[i]]  \n",
        "\n",
        "    shapes.append(shapes_i)\n",
        "\n",
        "  return shapes    \n",
        "\n",
        "# weights, bias = initialize_bias_weights(shapes)\n",
        "\n",
        "# num_layers = 2\n",
        "# Input_dim = 80\n",
        "# M = 30\n",
        "# process = 'concat'\n",
        "# Output_dim = [60,2]\n",
        "# shapes = initialize_shape(num_layers, Input_dim, Output_dim, M, process)\n",
        "# print(shapes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kc7pYsV8Vjd",
        "colab_type": "text"
      },
      "source": [
        "### Generate Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6iAWWi8ZRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def reshape_data(data):\n",
        "  N,M,F,V = data.shape\n",
        "  data_reshape = np.zeros([N,V,M,F])\n",
        "  for i in range(N):\n",
        "    X_i = data[i,:,:,:]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data_reshape[i,j,:,:] = X_ij\n",
        "\n",
        "  return data_reshape    \n",
        "\n",
        "def reshape_data_back(data):\n",
        "  N,V,M,F = data.shape\n",
        "  data_reshape = np.zeros([N,M,F,V])\n",
        "  for i in range(N):\n",
        "    X_i = data[i,:,:,:]\n",
        "    for v in range(V):\n",
        "      X_ij = X_i[v,:,:]\n",
        "      data_reshape[i,:,:,v] = X_ij\n",
        "\n",
        "  return data_reshape  \n",
        "\n",
        "\n",
        "def load_data_only(data_name):\n",
        "  # load data from google drive\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  N = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  data = np.zeros([N,M,F,V])\n",
        "  for i in range(N):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[i,:,:,j] = X_ij\n",
        "\n",
        "  return data  \n",
        "\n",
        "def add_noise(data, view, sigma):\n",
        "  [N,M,F,V] = data.shape;\n",
        "  X = data.copy()\n",
        "  if(view<V):\n",
        "    X_v = data[:,:,:,view]\n",
        "    s = np.random.normal(0, sigma, [N,M,F])\n",
        "    X_corrupted = X_v + np.array(s)\n",
        "    # print(X_corrupted - X_v)\n",
        "    X[:,:,:,view] = X_corrupted\n",
        "  \n",
        "  elif(view == V):\n",
        "    print('adding noise to both views')\n",
        "    X_v1 = data[:,:,:,0]\n",
        "    X_v2 = data[:,:,:,1]\n",
        "\n",
        "    s1 = np.random.normal(0, sigma, [N,M,F])\n",
        "    X_corrupted1 = X_v1 + np.array(s1)\n",
        "\n",
        "    s2 = np.random.normal(0, sigma, [N,M,F])\n",
        "    X_corrupted2 = X_v2 + np.array(s2)\n",
        "\n",
        "    X[:,:,:,0] = X_corrupted1\n",
        "    X[:,:,:,1] = X_corrupted2\n",
        "\n",
        "  return X\n",
        "\n",
        "# X = np.random.rand(3,3,2,2)\n",
        "# X_noise = add_noise(X, 0, 0.1)\n",
        "# print(X - X_noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAZX1cpaZcgE",
        "colab_type": "text"
      },
      "source": [
        "### GCN.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdFnJy5dTjxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def feature_process(X, process = 'concat'):\n",
        "  if(process == 'concat'):\n",
        "    H,W = X.shape\n",
        "    X_v = tf.reshape(X, [H*W,])\n",
        "\n",
        "  if(process == 'avg'):\n",
        "    X_v = tf.reduce_mean(X,axis = 0)\n",
        "\n",
        "  if(process == 'max'):\n",
        "    X_v = tf.reduce_max(X,axis = 0)  \n",
        "    \n",
        "  return X_v  \n",
        "  \n",
        "       \n",
        "def Graphconvolution_all(A, x, dropout, process):\n",
        "  if(isspmatrix(A)):\n",
        "    A = A.todense()\n",
        "\n",
        "  num_layers = len(weights)-2\n",
        "  if(len(weights) != len(bias)):\n",
        "    print('length error of initialization')\n",
        " \n",
        "  tf.random.set_seed(0)\n",
        "  input = tf.nn.dropout(x, dropout)\n",
        "  # input = x\n",
        "\n",
        "  for i in range(num_layers):\n",
        "\n",
        "    output = tf.matmul(input, weights[i])\n",
        "    output = tf.matmul(A, output)\n",
        "    output = output + bias[i]\n",
        "\n",
        "    output = tf.nn.relu(output)\n",
        "    input = output\n",
        "  \n",
        "  output = feature_process(output, process)    \n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def view_pooling_func(X, view_pooling):\n",
        "\n",
        "  if(view_pooling == 'avg'):\n",
        "    X_pooling = tf.reduce_mean(X,axis = 0)\n",
        "\n",
        "  if(view_pooling == 'max'): \n",
        "    X_pooling = tf.reduce_max(X,axis = 0)\n",
        "\n",
        "  if(view_pooling == 'concat'): \n",
        "    [V,F] = X.shape\n",
        "    X_pooling = tf.reshape(X,[1,V*F])\n",
        "\n",
        "\n",
        "  return X_pooling  \n",
        "\n",
        "\n",
        "def MV_model_infer(inputs_all, A, dropout, process, view_pooling):\n",
        "  N, V, M, F = inputs_all.shape\n",
        "  outputs_all = []\n",
        "  for i in range(N):\n",
        "    outputs_all_v = []\n",
        "    for v in range(V):\n",
        "      input_i_v = inputs_all[i,v,:,:]\n",
        "      output_i = Graphconvolution_all(A,input_i_v, dropout, process)\n",
        "      outputs_all_v.append(output_i)\n",
        "\n",
        "    output_feature_all_v = tf.convert_to_tensor(outputs_all_v)\n",
        "    View_feature_v = view_pooling_func(output_feature_all_v, view_pooling)\n",
        "    # View_feature_v = 0.018*output_feature_all_v[0,:] + 0.999 * output_feature_all_v[1,:] ## For HIV\n",
        "    #View_feature_v = 0.0089*output_feature_all_v[0,:] + 0.999 * output_feature_all_v[1,:] ## For BP\n",
        "\n",
        "    outputs_all.append(View_feature_v)\n",
        "\n",
        "  outputs_all = tf.convert_to_tensor(outputs_all)\n",
        "  \n",
        "  # if(process == 'concat'):\n",
        "  #   _,F_out = outputs_all.shape\n",
        "  #   outputs_all = tf.reshape(outputs_all, [int(N * M), int(F_out/M)])\n",
        "  #   outputs_all = tf.nn.l2_normalize(outputs_all, axis=1, epsilon=1e-12, name=None)\n",
        "  #   outputs_all = tf.reshape(outputs_all, [int(N), int(F_out)])\n",
        "  outputs_all = tf.nn.dropout(outputs_all, dropout)\n",
        "\n",
        "  outputs_all = tf.matmul(outputs_all, weights[-2]) + bias[-2]\n",
        "  outputs_all = tf.nn.relu(outputs_all)\n",
        "\n",
        "  logits  = tf.matmul(outputs_all, weights[-1]) + bias[-1]\n",
        "\n",
        "  return logits  \n",
        "\n",
        "\n",
        "def MV_model_infer_concat(inputs_all, A, dropout, process, view_pooling):\n",
        "  N, V, M, F = inputs_all.shape\n",
        "  for i in range(N):\n",
        "    for v in range(V):\n",
        "      input_i_v = inputs_all[i,v,:,:]\n",
        "      output_i = Graphconvolution_all(A,input_i_v, dropout, process)\n",
        "      output_i = tf.expand_dims(output_i,0)\n",
        "      if(v == 0):\n",
        "        outputs_all_v = output_i\n",
        "      else:\n",
        "        outputs_all_v = tf.concat([outputs_all_v,output_i], axis = 0)\n",
        "\n",
        "    View_feature_v = view_pooling_func(outputs_all_v, view_pooling)\n",
        "    View_feature_v = tf.expand_dims(View_feature_v,0)\n",
        "\n",
        "    if(i == 0):\n",
        "      outputs_all = View_feature_v\n",
        "    else:\n",
        "      outputs_all = tf.concat([outputs_all,View_feature_v], axis = 0)\n",
        "\n",
        "  outputs_all = tf.nn.dropout(outputs_all, dropout)\n",
        "\n",
        "  logits  = tf.matmul(outputs_all, weights[-1]) + bias[-1]\n",
        "\n",
        "  return logits  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmdTWfUqewgv",
        "colab_type": "text"
      },
      "source": [
        "### Train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuE1rHOtezei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss( logits , gt_labels ):\n",
        "    gt_labels = tf.stop_gradient(gt_labels) \n",
        "    return tf.nn.softmax_cross_entropy_with_logits(gt_labels,logits)\n",
        "   \n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "def batch_train(batch_inputs, batch_labels, A, dropout, process, view_pooling):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = MV_model_infer(batch_inputs, A, dropout, process, view_pooling)\n",
        "      # logits = model_infer(inputs_all, A, n_class, dropout, process)\n",
        "      current_loss = get_loss(logits, batch_labels)\n",
        "\n",
        "      trainable_variables = weights + bias\n",
        "\n",
        "      lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in trainable_variables])\n",
        "      lossL2_decay = tf.reduce_mean(current_loss) + 0.001*lossL2\n",
        "\n",
        "      grads = tape.gradient(lossL2_decay, trainable_variables)\n",
        "      # grads = tape.gradient(current_loss, trainable_variables)\n",
        "      \n",
        "      # Update the weights\n",
        "      optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "\n",
        "    avg_loss = tf.reduce_mean(current_loss)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def train_all(Inputs_train, train_labels, A, n_class, batch_size, n_epoch, dropout, process, view_pooling):\n",
        "\n",
        "  N_train = np.array(train_labels).size\n",
        "  num_batches_per_epoch = int((N_train - 1) / batch_size) + 1\n",
        "  ### Train\n",
        "  for iter in range(n_epoch):\n",
        "    epoch_loss = 0\n",
        "    idx_all = np.random.permutation(N_train)\n",
        "    start_idx = 0\n",
        "    for i in range(num_batches_per_epoch):\n",
        "      end_idx = start_idx + batch_size\n",
        "      if(end_idx > N_train):\n",
        "        end_idx = N_train \n",
        "      selected_idx = idx_all[start_idx:end_idx]\n",
        "\n",
        "      train_label_select = train_labels[selected_idx]\n",
        "      Inputs_select = Inputs_train[selected_idx,:,:,:]\n",
        "      train_label_select_one_hot = tf.one_hot(train_label_select , depth=n_class)\n",
        "      avg_loss = batch_train(Inputs_select, train_label_select_one_hot, A, dropout, process, view_pooling)\n",
        "      epoch_loss += avg_loss\n",
        "\n",
        "      start_idx += batch_size\n",
        "\n",
        "    if(iter % 10 == 0 or iter == n_epoch - 1):  \n",
        "      print('epoch = ',iter, 'loss = ', epoch_loss)\n",
        "\n",
        "def train_all_and_validate(Inputs_train, train_labels, val_data, val_labels, test_data, test_labels, A, n_class, batch_size, n_epoch, dropout, process, view_pooling):\n",
        "\n",
        "  N_train = np.array(train_labels).size\n",
        "  num_batches_per_epoch = int((N_train - 1) / batch_size) + 1\n",
        "  ### Train\n",
        "  for iter in range(n_epoch):\n",
        "    epoch_loss = 0\n",
        "    idx_all = np.random.permutation(N_train)\n",
        "    start_idx = 0\n",
        "    for i in range(num_batches_per_epoch):\n",
        "      end_idx = start_idx + batch_size\n",
        "      if(end_idx > N_train):\n",
        "        end_idx = N_train \n",
        "      selected_idx = idx_all[start_idx:end_idx]\n",
        "\n",
        "      train_label_select = train_labels[selected_idx]\n",
        "      Inputs_select = Inputs_train[selected_idx,:,:,:]\n",
        "      train_label_select_one_hot = tf.one_hot(train_label_select , depth=n_class)\n",
        "      avg_loss = batch_train(Inputs_select, train_label_select_one_hot, A, dropout, process, view_pooling)\n",
        "      epoch_loss += avg_loss\n",
        "\n",
        "      start_idx += batch_size\n",
        "\n",
        "    accuracy_val, auc_val, sensitivity, specificity = validate(val_data, val_labels, A, dropout, process, view_pooling)  \n",
        "\n",
        "    accuracy_test, auc_test, sensitivity, specificity = validate(test_data, test_labels, A, dropout, process, view_pooling)\n",
        "  \n",
        "    \n",
        "    print('epoch = ',iter, 'loss = ', epoch_loss, 'val accuracy = ', accuracy_val, 'test accuracy = ', accuracy_test)    \n",
        "\n",
        "\n",
        "def validate(Inputs_val, val_labels, A, dropout, process, view_pooling):\n",
        "  logits = MV_model_infer(Inputs_val, A, dropout, process, view_pooling)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  predicted_labels = np.argmax(predictions, axis = 1)\n",
        "  predictions_auc = np.max(predictions, axis = 1)\n",
        "\n",
        "  # print(predictions)\n",
        "  # print(predicted_labels)\n",
        "  # print(val_labels)\n",
        "\n",
        "  accuracy, auc, sensitivity, specificity = cal_metrics(val_labels, predicted_labels, predicted_labels)\n",
        "\n",
        "\n",
        "  return accuracy, auc, sensitivity, specificity\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leEAeQ63SOrE",
        "colab_type": "text"
      },
      "source": [
        "### Cal_metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgMBvdTSS0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn\n",
        "\n",
        "def cal_metrics(gt_label, predicted_label, predictions):\n",
        "  if(np.array(predicted_label).size != np.array(gt_label).size):\n",
        "    print('wrong label size')\n",
        "    print('wrong label size')\n",
        "    print('wrong label size')\n",
        "    print('wrong label size')\n",
        "    print('wrong label size')\n",
        "    print('wrong label size')\n",
        "\n",
        "\n",
        "  fpr, tpr, _ = sklearn.metrics.roc_curve(gt_label, predictions)\n",
        "  auc = 100 * sklearn.metrics.auc(fpr, tpr)\n",
        "  tn, fp, fn, tp = confusion_matrix(gt_label, predicted_label).ravel()\n",
        "  sensitivity = tp/(tp+fn)\n",
        "  specificity = tn/(tn+fp) \n",
        "\n",
        "  sum_correct = 0\n",
        "  num_labels = np.array(predicted_label).size\n",
        "\n",
        "  for i in range(num_labels):\n",
        "    gt_i = gt_label[i]\n",
        "    predicted_i = predicted_label[i]\n",
        "    if(gt_i == predicted_i):\n",
        "      sum_correct += 1\n",
        "\n",
        "  accuracy = sum_correct/num_labels\n",
        "\n",
        "  return accuracy, auc, sensitivity, specificity\n",
        "\n",
        "\n",
        "def cal_intersection_union(array_1, array_2):\n",
        "  array_1 = np.array(array_1)\n",
        "  array_2 = np.array(array_2)\n",
        "\n",
        "  intersect = np.intersect1d(array_1, array_2)\n",
        "\n",
        "  num_intersect = intersect.size\n",
        "  if(num_intersect>0):\n",
        "    print('************************************')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('oho, there is intersection')\n",
        "    print('************************************')\n",
        "\n",
        "  union = np.union1d(array_1, array_2)\n",
        "  num_union = union.size\n",
        "\n",
        "  union_array = np.array(range(0, num_union))\n",
        "\n",
        "  error = np.sum(union - union_array)\n",
        "\n",
        "  if(error > 0):\n",
        "    print('************************************')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('oho, wrong union')\n",
        "    print('************************************')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMcLAK5xQO_n",
        "colab_type": "text"
      },
      "source": [
        "### Train HOSVD Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb2GN2E_QQu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ad4714-7229-4c26-9f24-1cee0c01587d"
      },
      "source": [
        "!pip install tensorly\n",
        "import tensorly as tl\n",
        "import time\n",
        "from  scipy import *\n",
        "\n",
        "def train_U_with_noise(data, R, train_idx):\n",
        "  data_chosen = data[train_idx,:,:,:]\n",
        "  X1 = tl.unfold(data_chosen, mode=1)\n",
        "  B = np.matmul(X1,X1.transpose())\n",
        "  U, S, V = np.linalg.svd(B, full_matrices=True)\n",
        "  S = np.sqrt(S)\n",
        "  sum_all_S = np.sum(S)\n",
        "  len_S = len(S)\n",
        "  sum_part_S = 0\n",
        "  for i in range(len_S):\n",
        "    sum_part_S = sum_part_S + S[i]\n",
        "    if(sum_part_S>sum_all_S*R):\n",
        "        break\n",
        "\n",
        "  U = U[:,0:i]\n",
        "  return U\n",
        "\n",
        "def train_U_new(data, R):\n",
        "  N,V,M,F = data.shape\n",
        "  sum_X = 0 \n",
        "  for i in range(N):\n",
        "    for v in range(V):\n",
        "      X_iv = data[i,v,:,:]\n",
        "      sum_X = sum_X + np.matmul(X_iv, X_iv.transpose())\n",
        "\n",
        "  U, S, V = np.linalg.svd(sum_X, full_matrices=True)   \n",
        "  S = np.sqrt(S)\n",
        "  sum_all_S = np.sum(S)\n",
        "  len_S = len(S)\n",
        "  sum_part_S = 0\n",
        "  for i in range(len_S):\n",
        "    sum_part_S = sum_part_S + S[i]\n",
        "    if(sum_part_S>sum_all_S*R):\n",
        "        break\n",
        "  if(R == 1):\n",
        "    U = U\n",
        "  else:   \n",
        "    U = U[:,0:i]\n",
        "\n",
        "  return U   \n",
        "\n",
        "def train_U(data_name, R):\n",
        "  if(data_name == 'BP'):\n",
        "    x = scipy.io.loadmat('BP.mat')\n",
        "    X_normalize = x['X_normalize']\n",
        "\n",
        "  elif(data_name == 'HIV'):\n",
        "    x = scipy.io.loadmat('HIV.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  elif(data_name == 'PPMI'):\n",
        "    x = scipy.io.loadmat('PPMI.mat') \n",
        "    X_normalize = x['X'] \n",
        "\n",
        "  N = X_normalize.size\n",
        "  X_1 = X_normalize[0][0]\n",
        "  M,F,V = X_1.shape\n",
        "  # print('loading data:',data_name,'of size:',N,V,M,F)\n",
        "  data = np.zeros([N,M,F,V])\n",
        "  for i in range(N):\n",
        "    X_i = X_normalize[i][0]\n",
        "    for j in range(V):\n",
        "      X_ij = X_i[:,:,j]\n",
        "      data[i,:,:,j] = X_ij\n",
        "\n",
        "  data_select = data\n",
        "  X1 = tl.unfold(data_select, mode=1)\n",
        "  B = np.matmul(X1,X1.transpose())\n",
        "  U, S, V = np.linalg.svd(B, full_matrices=True)\n",
        "  S = np.sqrt(S)\n",
        "  sum_all_S = np.sum(S)\n",
        "  len_S = len(S)\n",
        "  sum_part_S = 0\n",
        "  for i in range(len_S):\n",
        "    sum_part_S = sum_part_S + S[i]\n",
        "    if(sum_part_S>sum_all_S*R):\n",
        "        break\n",
        "  if(R == 1):\n",
        "    U = U\n",
        "  else:   \n",
        "    U = U[:,0:i]\n",
        "  return U\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorly in /usr/local/lib/python3.6/dist-packages (0.4.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.18.5)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from tensorly) (1.3.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KapUToa9X8HA",
        "colab_type": "text"
      },
      "source": [
        "### Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YWobqkr0OFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print (\"Local time :\", localtime)\n",
        "print('##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$###############$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$*****************')\n",
        "\n",
        "\n",
        "########## Initialization\n",
        "\n",
        "method = 'gcn'\n",
        "n_class = 2\n",
        "data_name = 'BP'\n",
        "\n",
        "data, _, _, _, train_index_all, test_index_all, label_all = load_data_my_new(data_name, 0)\n",
        "data = np.array(data, dtype= 'float32')\n",
        "label_all = label_all[:,0]\n",
        "N,n_views,M,F = data.shape\n",
        "\n",
        "index_set_all = np.array(range(N))\n",
        "R = 1\n",
        "\n",
        "node_features = train_U_new(data, R)\n",
        "U = node_features\n",
        "data = data_projection(data, U)\n",
        "data = np.array(data,dtype='float32')\n",
        "\n",
        "num_layers = 2\n",
        "\n",
        "process = 'concat'\n",
        "view_pooling = 'max'\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "iter = 0\n",
        "kfold = 10\n",
        "\n",
        "accuracy_total_sum = 0\n",
        "auc_total_sum = 0\n",
        "sensitivity_total_sum = 0\n",
        "specificity_total_sum = 0\n",
        "\n",
        "accuracy_total_record = []\n",
        "auc_total_record = []\n",
        "sensitivity_total_record = []\n",
        "specificity_total_record = []\n",
        "\n",
        "n_epoch = 3\n",
        "batch_size = 6\n",
        "\n",
        "########################## Finish Initialization #####################\n",
        "\n",
        "for train_fold_chosen in range(kfold):\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  print('start training the model of fold:',train_fold_chosen)\n",
        "  print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "  _, _, indexes_set, labels_set, _, _, _ = load_data_my_new(data_name, train_fold_chosen)\n",
        "  iter = 0\n",
        "  indexes = indexes_set[0]\n",
        "  labels = labels_set[0]\n",
        "\n",
        "  accuracy_record = []\n",
        "  param_record = {}\n",
        "\n",
        "  train_idx,val_idx,_ = indexes\n",
        "  train_idx = train_index_all[train_fold_chosen]\n",
        "  train_labels, val_labels = label_all[train_idx], label_all[val_idx]\n",
        "  train_data,val_data = data[train_idx,:,:,:], data[val_idx,:,:,:]\n",
        "  test_index = test_index_all[train_fold_chosen]\n",
        "  test_data, test_labels = data[test_index,:,:,:], label_all[test_index]\n",
        "  cal_intersection_union(train_idx, test_index)\n",
        "\n",
        "  for Out_dim1 in range(30,50,20):\n",
        "      if(num_layers == 3):\n",
        "        Input_dim = F\n",
        "        o_dim1 = Out_dim1\n",
        "        o_dim2 = o_dim1\n",
        "        o_dim3 = n_class\n",
        "        Output_dim = [o_dim1,o_dim2,o_dim3]\n",
        "      elif(num_layers == 2):\n",
        "        Input_dim = F  \n",
        "        o_dim1 = Out_dim1\n",
        "        o_dim2 = n_class\n",
        "        Output_dim = [o_dim1,o_dim2]\n",
        "      shape_1 = [F, Out_dim1]\n",
        "      shape_2 = [M*Out_dim1, 1800]\n",
        "      shape_3 = [1800, n_class]\n",
        "      shapes = [] \n",
        "      shapes.append(shape_1)\n",
        "      shapes.append(shape_2)\n",
        "      shapes.append(shape_3) \n",
        "      weights, bias = initialize_bias_weights(shapes)\n",
        "\n",
        "      for knn in range(6,8,8):\n",
        "        A = obtain_normalized_adj(node_features,knn)\n",
        "        if(isspmatrix(A)):\n",
        "          A = A.todense()\n",
        "        A = np.array(A,dtype= 'float32')\n",
        "        train_all_and_validate(train_data, train_labels, val_data, val_labels, test_data, test_labels, A, n_class, batch_size, 39, dropout, process, view_pooling)\n",
        "        accuracy, auc, sensitivity, specificity = validate(test_data, test_labels, A, dropout, process, view_pooling)\n",
        "        print('Out_dim1 = ', Out_dim1, 'knn = ',knn, 'validate accuracy = ', accuracy, 'batch_size =', batch_size, 'dropout = ', dropout, 'process = ', process, 'view_pooling = ', view_pooling)\n",
        "\n",
        "\n",
        "\n",
        "  print('test fold = ', train_fold_chosen,' Out_dim1 = ', Out_dim1, 'knn = ',knn, \n",
        "        'accuracy = ', accuracy, 'auc = ', auc, 'sensitivity = ', sensitivity, 'specificity = ', specificity)\n",
        "\n",
        "  del weights\n",
        "  del bias\n",
        "\n",
        "\n",
        "  accuracy_total_sum = accuracy_total_sum + accuracy\n",
        "  auc_total_sum = auc_total_sum + auc\n",
        "  sensitivity_total_sum = sensitivity_total_sum + sensitivity\n",
        "  specificity_total_sum = specificity_total_sum + specificity\n",
        "\n",
        "  accuracy_total_record.append(accuracy)\n",
        "  auc_total_record.append(auc)\n",
        "  sensitivity_total_record.append(sensitivity)\n",
        "  specificity_total_record.append(specificity)\n",
        "\n",
        "avg_accuracy, avg_auc =  accuracy_total_sum/kfold, auc_total_sum/kfold\n",
        "avg_sensitivity, avg_specificity = sensitivity_total_sum/kfold, specificity_total_sum/kfold\n",
        "\n",
        "print('%%%%%%%%%%%%%%%%% ')\n",
        "print(' All Stats ')\n",
        "print('%%%%%%%%%%%%%%%%% ')\n",
        "print('avg_acc:', avg_accuracy, 'avg_auc:', avg_auc, 'avg_sensi:', avg_sensitivity, 'avg_speci:', avg_specificity)\n",
        "\n",
        "print(np.array(accuracy_total_record))\n",
        "print(np.array(auc_total_record))\n",
        "print(np.array(sensitivity_total_record))\n",
        "print(np.array(specificity_total_record))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAolDVjMp45w",
        "colab_type": "text"
      },
      "source": [
        "### Main New Iter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sWSjhKGp6ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print (\"Local time :\", localtime)\n",
        "print('##$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$###############$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$*****************')\n",
        "\n",
        "\n",
        "########## Initialization\n",
        "\n",
        "method = 'gcn'\n",
        "n_class = 2\n",
        "data_name = 'BP'\n",
        "\n",
        "data, _, _, _, train_index_all, test_index_all, label_all = load_data_my_new(data_name, 0)\n",
        "data = np.array(data, dtype= 'float32')\n",
        "label_all = label_all[:,0]\n",
        "N,n_views,M,F = data.shape\n",
        "\n",
        "index_set_all = np.array(range(N))\n",
        "R = 1\n",
        "\n",
        "node_features = train_U_new(data, R)\n",
        "\n",
        "U = node_features\n",
        "data = data_projection(data, U)\n",
        "data = np.array(data,dtype='float32')\n",
        "\n",
        "N,n_views,M,F = data.shape\n",
        "\n",
        "\n",
        "num_layers = 2\n",
        "\n",
        "process = 'concat'\n",
        "view_pooling = 'max'\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "iter = 0\n",
        "kfold = 10\n",
        "\n",
        "accuracy_total_record = []\n",
        "auc_total_record = []\n",
        "sensitivity_total_record = []\n",
        "specificity_total_record = []\n",
        "\n",
        "n_epoch = 3\n",
        "batch_size = 6\n",
        "\n",
        "acc_iter_all = 0\n",
        "auc_iter_all = 0\n",
        "sensi_iter_all = 0\n",
        "speci_iter_all = 0\n",
        "########################## Finish Initialization #####################\n",
        "for iter in range(1):\n",
        "  accuracy_total_sum = 0\n",
        "  auc_total_sum = 0\n",
        "  sensitivity_total_sum = 0\n",
        "  specificity_total_sum = 0\n",
        "  for train_fold_chosen in range(kfold):\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    print('start training the model of fold:',train_fold_chosen)\n",
        "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "    _, _, indexes_set, labels_set, _, _, _ = load_data_my_new(data_name, train_fold_chosen)\n",
        "    indexes = indexes_set[0]\n",
        "    labels = labels_set[0]\n",
        "\n",
        "    accuracy_record = []\n",
        "    param_record = {}\n",
        "\n",
        "    train_idx,val_idx,_ = indexes\n",
        "    train_labels, val_labels,_ = labels\n",
        "    train_data,val_data = data[train_idx,:,:,:], data[val_idx,:,:,:]\n",
        "    test_index = test_index_all[train_fold_chosen]\n",
        "    test_data, test_labels = data[test_index,:,:,:], label_all[test_index]\n",
        "\n",
        "    for Out_dim1 in range(30,50,20):\n",
        "        if(num_layers == 3):\n",
        "          Input_dim = F\n",
        "          o_dim1 = Out_dim1\n",
        "          o_dim2 = o_dim1\n",
        "          o_dim3 = n_class\n",
        "          Output_dim = [o_dim1,o_dim2,o_dim3]\n",
        "        elif(num_layers == 2):\n",
        "          Input_dim = F  \n",
        "          o_dim1 = Out_dim1\n",
        "          o_dim2 = n_class\n",
        "          Output_dim = [o_dim1,o_dim2]\n",
        "        shape_1 = [F, Out_dim1]\n",
        "        shape_2 = [M*Out_dim1, 1800]\n",
        "        shape_3 = [1800, n_class]\n",
        "        shapes = [] \n",
        "        shapes.append(shape_1)\n",
        "        shapes.append(shape_2)\n",
        "        shapes.append(shape_3) \n",
        "\n",
        "\n",
        "        for knn in range(6,8,2):\n",
        "          A = obtain_normalized_adj(node_features,knn)\n",
        "\n",
        "    print('##################################################################')\n",
        "    print('start Retraining and testing the model with fold:',train_fold_chosen)\n",
        "    print('Out_dim1 = ', Out_dim1, 'knn = ',knn, 'batch_size =', batch_size, 'dropout = ', dropout, 'process = ', process, 'view_pooling = ', view_pooling)\n",
        "    print('##################################################################')\n",
        "\n",
        "\n",
        "    train_index = train_index_all[train_fold_chosen]\n",
        "    train_data, train_labels = data[train_index,:,:,:], label_all[train_index]\n",
        "    print('train_data_shape = ', train_data.shape)\n",
        "    cal_intersection_union(train_index, test_index)\n",
        "\n",
        "    print('# of test: ', np.array(test_index).size)\n",
        "\n",
        "    weights, bias = initialize_bias_weights(shapes)\n",
        "\n",
        "    A = obtain_normalized_adj(node_features, knn)\n",
        "    if(isspmatrix(A)):\n",
        "      A = A.todense()\n",
        "    A = np.array(A,dtype= 'float32')  \n",
        "\n",
        "    train_all_and_validate(train_data, train_labels, val_data, val_labels, test_data, test_labels, A, n_class, batch_size, 39, dropout, process, view_pooling)\n",
        "\n",
        "    accuracy, auc, sensitivity, specificity = validate(test_data, test_labels, A, dropout, process, view_pooling)\n",
        "\n",
        "    print('test fold = ', train_fold_chosen,' Out_dim1 = ', Out_dim1, 'knn = ',knn, \n",
        "          'accuracy = ', accuracy, 'auc = ', auc, 'sensitivity = ', sensitivity, 'specificity = ', specificity)\n",
        "\n",
        "    del weights\n",
        "    del bias\n",
        "\n",
        "    accuracy_total_sum = accuracy_total_sum + accuracy\n",
        "    auc_total_sum = auc_total_sum + auc/100\n",
        "    sensitivity_total_sum = sensitivity_total_sum + sensitivity\n",
        "    specificity_total_sum = specificity_total_sum + specificity\n",
        "\n",
        "  avg_accuracy, avg_auc =  accuracy_total_sum/kfold, auc_total_sum/kfold\n",
        "  avg_sensitivity, avg_specificity = sensitivity_total_sum/kfold, specificity_total_sum/kfold\n",
        "\n",
        "  print('iter = ', iter, 'avg_acc:', avg_accuracy, 'avg_auc:', avg_auc, 'avg_sensi:',avg_sensitivity, 'avg_speci:', avg_specificity)\n",
        "  \n",
        "  acc_iter_all += avg_accuracy\n",
        "  auc_iter_all += avg_auc\n",
        "  sensi_iter_all += avg_sensitivity\n",
        "  speci_iter_all += avg_specificity\n",
        "\n",
        "  accuracy_total_record.append(avg_accuracy)\n",
        "  auc_total_record.append(avg_auc)\n",
        "  sensitivity_total_record.append(avg_sensitivity)\n",
        "  specificity_total_record.append(avg_specificity)\n",
        "\n",
        "print('%%%%%%%%%%%%%%%%%')\n",
        "print(' All Stats ')\n",
        "print('%%%%%%%%%%%%%%%%%')\n",
        "print('avg_acc:', acc_iter_all/(iter+1), 'avg_auc:', auc_iter_all/(iter+1), 'avg_sensi:', sensi_iter_all/(iter+1), 'avg_speci:', speci_iter_all/(iter+1))\n",
        "\n",
        "print('%%%%%%%%%%%%%%%%%')\n",
        "print(' All Stats ')\n",
        "print('%%%%%%%%%%%%%%%%% ')\n",
        "print('acc = ', accuracy_total_record)\n",
        "print('auc = ', auc_total_record)\n",
        "print('sensi = ', sensitivity_total_record)\n",
        "print('speci = ', specificity_total_record)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}